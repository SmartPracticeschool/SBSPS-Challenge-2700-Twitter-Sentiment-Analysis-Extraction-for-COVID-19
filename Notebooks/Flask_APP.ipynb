{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flask_APP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmartPracticeschool/SBSPS-Challenge-2700-Twitter-Sentiment-Analysis-Extraction-for-COVID-19/blob/master/Notebooks/Flask_APP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCfYw4aTPsIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6256932f-530a-4ac1-824a-47662b1361e0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmLBU7cYQ1dp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e6526c58-a093-4e96-88e9-1deb4c45ca65"
      },
      "source": [
        "!pip install -q transformers==2.1.1\n",
        "!pip install -q torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10kB 33.4MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 296kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 17.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wScUylirULeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEVWrYEjUahG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q flask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tLhKX_GUhYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6b6ab722-2320-4169-ab01-c2ec8ba61252"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/15/1c026f3aeafd26db30cb633d9915aae666a415179afa5943263e5dbd55a6/tokenizers-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 8.5MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv3RUMkEUqNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install flask-socketio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pxPnElpvWyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install flask_restful"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtmPKbgwQ-aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path \n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import os\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import random \n",
        "\n",
        "# fastai\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "\n",
        "# transformers\n",
        "from transformers import *\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import tokenizers\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import bigrams\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "eng_stopwords = stopwords.words('english')\n",
        "import collections\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import networkx as nx\n",
        "\n",
        "from tweepy import OAuthHandler\n",
        "#from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from flask import Flask, request, jsonify, make_response\n",
        "from flask import render_template, url_for, flash, redirect\n",
        "from flask_socketio import SocketIO, emit\n",
        "from threading import Thread, Event\n",
        "\n",
        "from flask_restful import reqparse, abort, Api, Resource\n",
        "import json\n",
        "from flask import jsonify"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLmnol4BVEUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app = Flask(__name__)\n",
        "app.config['SECRET_KEY'] = '5791628bb0b13ce0c676dfde280ba012'\n",
        "socketio = SocketIO(app)\n",
        "\n",
        "# Twitter credentials\n",
        "consumer_key = ''\n",
        "consumer_secret = ''\n",
        "access_key = ''\n",
        "access_secret = ''\n",
        "\n",
        "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0LWwIEgW2al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "            tokens = [CLS] + tokens + [SEP]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "            if self.model_type in ['xlnet']:\n",
        "                tokens = tokens + [SEP] +  [CLS]\n",
        "            else:\n",
        "                tokens = [CLS] + tokens + [SEP]\n",
        "        return tokens\n",
        "\n",
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n",
        "    \n",
        "    def __getstate__(self):\n",
        "        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n",
        "\n",
        "    def __setstate__(self, state:dict):\n",
        "        self.itos = state['itos']\n",
        "        self.tokenizer = state['tokenizer']\n",
        "        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})\n",
        "\n",
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        # attention_mask\n",
        "        # Mask to avoid performing attention on padding token indices.\n",
        "        # Mask values selected in ``[0, 1]``:\n",
        "        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                  attention_mask = attention_mask)[0]   \n",
        "        return logits\n",
        "\n",
        "def predict_sentiment(text):\n",
        "  sentiment = learner.predict(text)[1].item()\n",
        "  return sentiment\n",
        "\n",
        "def sentiment_label (Sentiment):\n",
        "   if Sentiment == 2:\n",
        "       return \"positive\"\n",
        "   elif Sentiment == 0:\n",
        "       return \"negative\"\n",
        "   else :\n",
        "       return \"neutral\"\n",
        "\n",
        "def replace_url(string): # cleaning of URL\n",
        "    text = re.sub(r'http\\S+', 'LINK', string)\n",
        "    return text\n",
        "\n",
        "\n",
        "def replace_email(text):#Cleaning of Email related text\n",
        "    line = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','MAIL',str(text))\n",
        "    return \"\".join(line)\n",
        "\n",
        "def rep(text):#cleaning of non standard words\n",
        "    grp = text.group(0)\n",
        "    if len(grp) > 3:\n",
        "        return grp[0:2]\n",
        "    else:\n",
        "        return grp# can change the value here on repetition\n",
        "def unique_char(rep,sentence):\n",
        "    convert = re.sub(r'(\\w)\\1+', rep, sentence) \n",
        "    return convert\n",
        "\n",
        "def find_dollar(text):#Finding the dollar sign in the text\n",
        "    line=re.sub(r'\\$\\d+(?:\\.\\d+)?','PRICE',text)\n",
        "    return \"\".join(line)\n",
        "\n",
        "def replace_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'EMOJI', text) \n",
        "\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
        "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
        "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
        "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
        "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
        "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
        "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = str(text)\n",
        "    for punct in puncts + list(string.punctuation):\n",
        "        if punct in text:\n",
        "            text = text.replace(punct, f'')\n",
        "    return text\n",
        "   \n",
        "def replace_asterisk(text):\n",
        "    text = re.sub(\"\\*\", 'ABUSE ', text)\n",
        "    return text\n",
        "\n",
        "def remove_duplicates(text):\n",
        "    text = re.sub(r'\\b(\\w+\\s*)\\1{1,}', '\\\\1', text)\n",
        "    return text\n",
        "\n",
        "def change(text):\n",
        "    if(text == ''):\n",
        "        return text\n",
        "  #calling the subfunctions in the cleaning function\n",
        "    text = replace_email(text)\n",
        "    text = replace_url(text)\n",
        "    text = unique_char(rep,text)\n",
        "    text = replace_asterisk(text)\n",
        "    text = remove_duplicates(text)\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "\n",
        "def extract_tweets(search_words,date_since, date_until, numTweets):\n",
        "  return(tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, until=date_until, tweet_mode='extended').items(numTweets))\n",
        "\n",
        "def scraptweets(search_words, date_since, date_until, numTweets, numRuns):\n",
        "    # Define a pandas dataframe to store the date:\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following', 'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount', 'text', 'hashtags'])\n",
        "    db_tweets['hashtags'] = db_tweets['hashtags'].astype('object')\n",
        "    #db_tweets = pd.DataFrame()\n",
        "\n",
        "    for i in range(numRuns):\n",
        "\n",
        "        tweets = extract_tweets(search_words,date_since,date_until,numTweets)\n",
        "        # Store these tweets into a python list\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "        print(len(tweet_list))\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "            username = tweet.user.screen_name\n",
        "            acctdesc = tweet.user.description\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "            lst=[]\n",
        "            for h in hashtags:\n",
        "                lst.append(h['text'])\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            itweet = [username,acctdesc,location,following,followers,totaltweets,usercreatedts,tweetcreatedts,retweetcount,text,lst]\n",
        "            db_tweets.loc[len(db_tweets)] = itweet\n",
        "\n",
        "            noTweets += 1\n",
        "            print(noTweets)\n",
        "\n",
        "            #filename = \"tweets.csv\"\n",
        "            #with open(filename, \"a\", newline='') as fp:\n",
        "             #   wr = csv.writer(fp, dialect='excel')\n",
        "              #  wr.writerow(itweet)\n",
        "\n",
        "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
        "        if i+1 != numRuns:\n",
        "            time.sleep(920)\n",
        "\n",
        "        filename = \"/content/analysis.csv\"\n",
        "        db_tweets['text'] = db_tweets['text'].apply(change)\n",
        "        db_tweets = db_tweets[['retweetcount', 'text']]\n",
        "        # Store dataframe in csv with creation date timestamp\n",
        "        db_tweets.drop_duplicates(subset =\"text\", keep = 'first', inplace = True)\n",
        "        db_tweets.to_csv(filename, index = False) #\n",
        "\n",
        "# Functions for Sentiment Extractor\n",
        "def save_weights(model, dst_fn):\n",
        "    weights = model.get_weights()\n",
        "    with open(dst_fn, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "\n",
        "\n",
        "def load_weights(model, weight_fn):\n",
        "    with open(weight_fn, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    return model\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # adjust the targets for sequence bucketing\n",
        "    ll = tf.shape(y_pred)[1]\n",
        "    y_true = y_true[:, :ll]\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
        "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
        "\n",
        "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
        "    max_len = tf.reduce_max(lens)\n",
        "    ids_ = ids[:, :max_len]\n",
        "    att_ = att[:, :max_len]\n",
        "    tok_ = tok[:, :max_len]\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
        "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "    \n",
        "    # this is required as `model.predict` needs a fixed size!\n",
        "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    \n",
        "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
        "    return model, padded_model\n",
        "\n",
        "def generate_wordcloud(data,title):\n",
        "  wc = WordCloud(width=400, height=330, max_words=150,colormap=\"Dark2\",background_color='white', collocations=False).generate_from_frequencies(data)\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.imshow(wc, interpolation='bilinear')\n",
        "  plt.tight_layout(pad=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TteKYo5-W7ZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route(\"/\",methods=['GET', 'POST'])\n",
        "@app.route(\"/home\", methods=['GET', 'POST'])\n",
        "def home():\n",
        "    return render_template('/content/dashboard.html')\n",
        "\n",
        "@app.route(\"/live_count\", methods=['GET', 'POST'])\n",
        "def live_count():\n",
        "    return render_template('/content/live_case_count.html')\n",
        "\n",
        "@app.route(\"/about\", methods=['GET', 'POST'])\n",
        "def about():\n",
        "    return render_template('/content/about_us.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjUM5DkKbiwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOUR = 3600;\n",
        "\n",
        "thread = Thread()\n",
        "thread_stop_event = Event()\n",
        "\n",
        "class DailyUpdate():\n",
        "    def __init__(self):\n",
        "        self.delay = 24*HOUR\n",
        "        super(DailyUpdate, self).__init__()\n",
        "    def plotGenerator(self):\n",
        "        \"\"\"\n",
        "        Generates real time plots every 1 day.\n",
        "        \"\"\"\n",
        "        while not thread_stop_event.isSet():\n",
        "            # Initialise these variables:\n",
        "\n",
        "            search_words = \"(#India AND #COVID-19) OR #COVID19India\"\n",
        "            yesterday = datetime.datetime.now() - datetime.timedelta(days = 1)\n",
        "            date_since = yesterday.strftime(\"%Y-%m-%d\")\n",
        "            date_until = datetime.datetime.today().strftime('%Y-%m-%d')\n",
        "            numTweets = 2500\n",
        "            numRuns = 1\n",
        "            # Call the function scraptweets\n",
        "            program_start = time.time()\n",
        "            scraptweets(search_words, date_since, date_until, numTweets, numRuns)\n",
        "            program_end = time.time()\n",
        "\n",
        "            model_type = 'roberta'\n",
        "            pretrained_model_name = 'roberta-base'\n",
        "\n",
        "            model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "            transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "            transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "            fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
        "\n",
        "            pad_idx = transformer_tokenizer.pad_token_id\n",
        "\n",
        "            p = '/content/drive/My Drive/IBM_Hackathon_2020/Roberta_Model'\n",
        "            learner = load_learner(p, 'transformer.pkl')\n",
        "\n",
        "            path = '/content/analysis.csv'\n",
        "            predictions = pd.read_csv(path)\n",
        "\n",
        "            predictions['Prediction'] = predictions['text'].apply(predict_sentiment)\n",
        "            predictions['Prediction'] = predictions['Prediction'].apply(sentiment_label)\n",
        "            class_names = ['negative','positive','neutral']\n",
        "\n",
        "            predictions.rename(columns={'Prediction':'sentiment'},inplace=True)\n",
        "\n",
        "            MAX_LEN = 310\n",
        "            PAD_ID = 1\n",
        "            num_splits = 5\n",
        "            SEED = 88888\n",
        "\n",
        "            PATH = '/content/gdrive/My Drive/IBM_Hackathon_2020/Tf-Roberta/'\n",
        "            tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "                vocab_file=PATH+'vocab-roberta-base.json', \n",
        "                merges_file=PATH+'merges-roberta-base.txt', \n",
        "                lowercase=True,\n",
        "                add_prefix_space=True\n",
        "            )\n",
        "\n",
        "            test = predictions\n",
        "            test['len'] = test['text'].str.len()\n",
        "            test = test[test['len']<=310]\n",
        "            test.drop(\"len\",axis=1,inplace=True)\n",
        "            test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            ct = test.shape[0]\n",
        "            input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "            attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "            token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "            sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "\n",
        "            for k in range(test.shape[0]):\n",
        "                    \n",
        "                # INPUT_IDS\n",
        "                text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "                enc = tokenizer.encode(text1)                \n",
        "                s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "                input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "                attention_mask_t[k,:len(enc.ids)+3] = 1\n",
        "\n",
        "            DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "            preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "            preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "            for fold in range(0,5):\n",
        "              \n",
        "              K.clear_session()\n",
        "              model, padded_model = build_model()\n",
        "              path = '/content/gdrive/My Drive/IBM_Hackathon_2020/R_CNN_weights/'\n",
        "              weight_fn = path+'v0-roberta-%i.h5'%(fold)\n",
        "\n",
        "              print('Loading model...')\n",
        "              # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "              load_weights(model, weight_fn) \n",
        "              \n",
        "              print('Predicting Test...')\n",
        "\n",
        "              preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "              preds_start += preds[0]/num_splits\n",
        "              preds_end += preds[1]/num_splits\n",
        "\n",
        "            all = []\n",
        "            for k in range(input_ids_t.shape[0]):\n",
        "                a = np.argmax(preds_start[k,])\n",
        "                b = np.argmax(preds_end[k,])\n",
        "                if a>b: \n",
        "                    st = test.loc[k,'text']\n",
        "                else:\n",
        "                    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "                    enc = tokenizer.encode(text1)\n",
        "                    st = tokenizer.decode(enc.ids[a-2:b-1])\n",
        "                all.append(st)\n",
        "            \n",
        "            test['selected_text'] = all\n",
        "            test.to_csv('/content/analysis.csv',index=False)\n",
        "\n",
        "            data=pd.read_csv(\"/content/analysis.csv\")\n",
        "            df = data.sentiment.value_counts()\n",
        "            size = list(df.values)\n",
        "            names = list(df.index)\n",
        "            fig = plt.figure(figsize=(10,10))\n",
        "            plt.xlabel(\"Sentiment\",Fontsize = 16)\n",
        "            plt.ylabel(\"Frequency\",Fontsize = 16)\n",
        "            sns.barplot(names,size,alpha = 0.8)\n",
        "            fig.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/bar.png\")\n",
        "            \n",
        "            df_new = pd.DataFrame(dict(\n",
        "                r=list(df.values),\n",
        "                theta=list(df.index)))\n",
        "            plt.figure(figsize=(10,10))\n",
        "            fig = px.line_polar(df_new, r='r', theta='theta', line_close=True)\n",
        "            fig.update_traces(fill='toself')\n",
        "            fig.write_image(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/radar_plot.png\")\n",
        "\n",
        "            # Pie chart\n",
        "            labels = list(df.index)\n",
        "            sizes = list(df.values)\n",
        "            # only \"explode\" the 2nd slice \n",
        "            explode = (0.1, 0.1, 0.1)\n",
        "            #add colors\n",
        "            colors = ['#ff9999','#66b3ff','#99ff99']\n",
        "            fig1, ax1 = plt.subplots()\n",
        "            ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
        "                    shadow=True, startangle=90)\n",
        "            # Equal aspect ratio ensures that pie is drawn as a circle\n",
        "            ax1.axis('equal')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/pie_chart.png')\n",
        "\n",
        "            for i in range(3):\n",
        "              Data= data[data[\"sentiment\"]==df.index[i]]\n",
        "              Word_frequency = pd.Series(' '.join(Data.selected_text).split()).value_counts()[:20]#Calculating the words frequency\n",
        "              plt.figure(figsize=(25,10))\n",
        "              plt.ylabel(\"Frequency\",fontsize=16)\n",
        "              plt.title(\"Sentiment Triggers\")\n",
        "              sns.barplot(Word_frequency.index,Word_frequency.values,alpha=0.8)\n",
        "              plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/wordfrequency_\"+df.index[i]+\".png\")\n",
        "\n",
        "            for i in range(0,2):\n",
        "                Analysis_Data = data\n",
        "                data[\"selected_text\"]=data[\"selected_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))\n",
        "                Sentiment = Analysis_Data[Analysis_Data['sentiment'] == df.index[i]]#Creating the dataframe of having same sentiment\n",
        "                Word_frequency = pd.Series(' '.join(Sentiment.selected_text).split()).value_counts()[:50]#Calculating the words frequency\n",
        "                generate_wordcloud(Word_frequency.sort_values(ascending=False),data.index[i])\n",
        "                plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/Wordcloud_\" +df.index[i]+\" .png\")\n",
        "\n",
        "            data[\"text\"]=data[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))\n",
        "            bigrams = [b for l in data.text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df = pd.DataFrame(bigram_counts.most_common(10),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "            x =bigram_df.bigram\n",
        "            y = bigram_df.frequency\n",
        "\n",
        "            fig, ax = plt.subplots(1, 1, figsize = (20, 15), dpi=300)\n",
        "            sns.barplot(x,y,alpha=0.8)\n",
        "            plt.ylabel(\"Frequency\",fontsize=16)\n",
        "            ax.set_xlabel('')\n",
        "            plt.savefig('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/bigram_freq.png')\n",
        "\n",
        "            ext_data_negative = data[data[\"sentiment\"]=='negative']\n",
        "            ext_data_positive = data[data[\"sentiment\"]=='positive']\n",
        "            bigrams = [b for l in ext_data_positive.selected_text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df_positive = pd.DataFrame(bigram_counts.most_common(60),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "            bigrams = [b for l in ext_data_negative.selected_text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df_negative = pd.DataFrame(bigram_counts.most_common(80),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "\n",
        "            # Create network plot \n",
        "            G=nx.grid_2d_graph(2,2)\n",
        "\n",
        "            pos = nx.fruchterman_reingold_layout(G,k=10,iterations=100)\n",
        "            fig,ax = plt.subplots(figsize=(50,30)) \n",
        "            d = bigram_df_negative.set_index('bigram').T.to_dict('records')\n",
        "            for k, v in d[0].items():\n",
        "                G.add_edge(k[0], k[1], weight=(v * 10))\n",
        "            pos = nx.fruchterman_reingold_layout(G,k=10,iterations=100) \n",
        "              \n",
        "            nx.draw_networkx(G, pos,\n",
        "                            font_size=16,\n",
        "                            width=4,\n",
        "                            edge_color='#e25a4b',\n",
        "                            node_size=500,\n",
        "                            title = \"Negative Sentiment\",\n",
        "                            with_labels = False,\n",
        "                            ax=ax)\n",
        "            x_values, y_values = zip(*pos.values())\n",
        "            x_max = max(x_values)\n",
        "            x_min = min(x_values)\n",
        "            x_margin = (x_max - x_min) * 0.25\n",
        "            plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        "\n",
        "            for key, value in pos.items():\n",
        "                x, y = value[0]+.135, value[1]+.045\n",
        "                ax.text(x, y,\n",
        "                        s=key,bbox=dict(facecolor='#ffcd94', alpha=0.4),\n",
        "                        horizontalalignment='center', fontsize=35)\n",
        "            plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/ext_negative.png\")        \n",
        "            fig,ax  = plt.subplots(figsize=(50,30))\n",
        "            d = bigram_df_positive.set_index('bigram').T.to_dict('records')\n",
        "            for k, v in d[0].items():\n",
        "                G.add_edge(k[0], k[1], weight=(v * 10))\n",
        "            pos = nx.fruchterman_reingold_layout(G,k=10,iterations=100) \n",
        "              \n",
        "            nx.draw_networkx(G, pos,\n",
        "                            font_size=16,\n",
        "                            width=4,\n",
        "                            edge_color='#999894',\n",
        "                            node_size=500,\n",
        "                            with_labels = False,\n",
        "                            title = \"Positve Sentiment\",\n",
        "                            ax=ax)\n",
        "            x_values, y_values = zip(*pos.values())\n",
        "            x_max = max(x_values)\n",
        "            x_min = min(x_values)\n",
        "            x_margin = (x_max - x_min) * 0.25\n",
        "            plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        "\n",
        "            # Create offset labels\n",
        "            for key, value in pos.items():\n",
        "                x, y = value[0]+.135, value[1]+.045\n",
        "                ax.text(x, y,\n",
        "                        s=key,bbox=dict(facecolor='#7c99d0', alpha=0.4),\n",
        "                        horizontalalignment='center', fontsize=35)\n",
        "            plt.savefig(\"/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/ext_positive.png\")\n",
        "\n",
        "            data[\"text\"]=data[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (eng_stopwords)]))\n",
        "            bigrams = [b for l in data.text for b in zip(l.split(\" \")[:-1], l.split(\" \")[1:])]\n",
        "            bigram_counts = collections.Counter(bigrams)\n",
        "            bigram_df = pd.DataFrame(bigram_counts.most_common(60),\n",
        "                                        columns=['bigram', 'frequency'])\n",
        "            \n",
        "            d = bigram_df.set_index('bigram').T.to_dict('records')\n",
        "            # Create network plot \n",
        "            G = nx.Graph()\n",
        "            for k, v in d[0].items():\n",
        "                G.add_edge(k[0], k[1], weight=(v * 10))\n",
        "\n",
        "            fig,ax = plt.subplots(figsize=(20,20))\n",
        "            pos = nx.spring_layout(G,dim=2,k=5)\n",
        "\n",
        "            # Plot networks\n",
        "            nx.draw_networkx(G, pos,\n",
        "                            font_size=12,\n",
        "                            width=4,\n",
        "                            edge_color='grey',\n",
        "                            node_color='#4a4140',\n",
        "                            node_size=500,\n",
        "                            with_labels = False,\n",
        "                            ax=ax)\n",
        "            x_values, y_values = zip(*pos.values())\n",
        "            x_max = max(x_values)\n",
        "            x_min = min(x_values)\n",
        "            x_margin = (x_max - x_min) * 0.25\n",
        "            plt.xlim(x_min - x_margin, x_max + x_margin)\n",
        "\n",
        "\n",
        "            # Create offset labels\n",
        "            for key, value in pos.items():\n",
        "                x, y = value[0]+.135, value[1]+.045\n",
        "                ax.text(x, y,\n",
        "                        s=key,\n",
        "                        bbox=dict(facecolor='#ffcd94', alpha=0.4),\n",
        "                        horizontalalignment='center', fontsize=25)\n",
        "                \n",
        "            fig.savefig('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/network.png')\n",
        "\n",
        "            fig = px.box(data, y=\"retweetcount\",points=\"all\")\n",
        "            fig.update_layout(\n",
        "                yaxis_title=\"Retweet Count\",\n",
        "                font=dict(\n",
        "                    family=\"Courier New, monospace\",\n",
        "                    size=18,\n",
        "                    color=\"#7f7f7f\"\n",
        "                )\n",
        "            )\n",
        "            fig.write_image('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/retweet_count_boxplot.png')\n",
        "\n",
        "            fig = px.box(data, y=\"sentiment\",points=\"all\")\n",
        "            fig.update_layout(\n",
        "                yaxis_title=\"Sentiment\",\n",
        "                font=dict(\n",
        "                    family=\"Courier New, monospace\",\n",
        "                    size=18,\n",
        "                    color=\"#7f7f7f\"\n",
        "                )\n",
        "            )\n",
        "            fig.write_image('/content/drive/My Drive/IBM_Hackathon_2020/Real-time-Data/Images/sentiment_boxplot.png')\n",
        "\n",
        "            # socketio.emit('newnumber', {'number': number}, namespace='/test')\n",
        "            sleep(self.delay)\n",
        "    def run(self):\n",
        "        self.plotGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CJhc7mzqzY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/realtime', methods=['GET', 'POST'])\n",
        "def realtime():\n",
        "    #only by sending this page first will the client be connected to the socketio instance\n",
        "    return render_template('/content/real_time_analysis.html')\n",
        "\n",
        "@socketio.on('connect', namespace='/analyze')\n",
        "def test_connect():\n",
        "    # need visibility of the global thread object\n",
        "    global thread\n",
        "    print('Client connected')\n",
        "    #Start the random number generator thread only if the thread has not been started before.\n",
        "    if not thread.isAlive():\n",
        "        print(\"Starting Thread\")\n",
        "        thread = DailyUpdate()\n",
        "        thread.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEvJSHTPt8JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api = Api(app)\n",
        "\n",
        "@app.route(\"/twitter\", methods=['GET', 'POST'])\n",
        "def twitter():\n",
        "    return render_template('/content/twitter_live_feed.html')\n",
        "\n",
        "class GetSenti(Resource):\n",
        "  def getsentiment(request):\n",
        "      data = {\"success\": False}\n",
        "      # if parameters are found, echo the msg parameter \n",
        "      if (request.data != None):  \n",
        "          with graph.as_default():\n",
        "              data[\"predictions\"] = predict(request.GET.get(\"text\"))\n",
        "          data[\"success\"] = True\n",
        "      return JsonResponse(data)\n",
        "\n",
        "api.add_resource(GetSenti, '/getsentiment', methods=['GET', 'POST'])\n",
        "\n",
        "class Analyze(Resource):\n",
        "  def analyzehashtag(request):\n",
        "      positive = 0\n",
        "      neutral = 0\n",
        "      negative = 0\n",
        "      \n",
        "      model_type = 'roberta'\n",
        "      pretrained_model_name = 'roberta-base'\n",
        "\n",
        "      model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "      transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "      transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "      fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
        "\n",
        "      pad_idx = transformer_tokenizer.pad_token_id\n",
        "\n",
        "      p = '/content/drive/My Drive/IBM_Hackathon_2020/Roberta_Model'\n",
        "      learner = load_learner(p, 'transformer.pkl')\n",
        "\n",
        "      for tweet in tweepy.Cursor(api.search,q=\"#\" + request.GET.get(\"text\") + \" -filter:retweets\",rpp=5,lang=\"en\", tweet_mode='extended').items(100):\n",
        "          with graph.as_default():\n",
        "              text = change(tweet.full_text)\n",
        "              prediction = predict_sentiment(text)\n",
        "              prediction = sentiment_label(prediction)\n",
        "          if(prediction == \"Positive\"):\n",
        "              positive += 1\n",
        "          if(prediction == \"Neutral\"):\n",
        "              neutral += 1\n",
        "          if(prediction == \"Negative\"):\n",
        "              negative += 1\n",
        "      return JsonResponse({\"positive\": positive, \"neutral\": neutral, \"negative\": negative});\n",
        "\n",
        "api.add_resource(Analyze, '/analyzehashtag', methods=['GET', 'POST'])\n",
        "\n",
        "class Gettweets(Resource):\n",
        "  def gettweets(request):\n",
        "      \n",
        "      model_type = 'roberta'\n",
        "      pretrained_model_name = 'roberta-base'\n",
        "\n",
        "      model_class, tokenizer_class, config_class = RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "\n",
        "      transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "      transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "      fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
        "\n",
        "      pad_idx = transformer_tokenizer.pad_token_id\n",
        "\n",
        "      p = '/content/drive/My Drive/IBM_Hackathon_2020/Roberta_Model'\n",
        "      learner = load_learner(p, 'transformer.pkl')\n",
        "\n",
        "      tweets = []\n",
        "\n",
        "      for tweet in tweepy.Cursor(api.search,q=\"#\" + request.GET.get(\"text\") + \" -filter:retweets\",rpp=5,lang=\"en\", tweet_mode='extended').items(50):\n",
        "          temp = {}\n",
        "          text = change(tweet.full_text)\n",
        "          temp[\"text\"] = text\n",
        "          temp[\"username\"] = tweet.user.screen_name\n",
        "          with graph.as_default():\n",
        "              text = change(tweet.full_text)\n",
        "              prediction = predict_sentiment(text)\n",
        "              prediction = sentiment_label(prediction)\n",
        "          temp[\"label\"] = prediction\n",
        "          tweets.append(temp)\n",
        "      return JsonResponse({\"results\": tweets});\n",
        "\n",
        "api.add_resource(Gettweets, '/gettweets', methods=['GET', 'POST'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbz88Zhs6rS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0',port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
